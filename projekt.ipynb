{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 3, 32, 32) containing float32\n",
      " - an array of size (250, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "train_batch_size = 100\n",
    "validation_batch_size = 250\n",
    "\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    #(Flatten, [], {'which_sources': 'features'}),\n",
    "    #(Flatten, [], {'which_sources': 'targets'}),\n",
    "    #(Mapping, [lambda batch: (b.T for b in batch)], {}) \n",
    "    )\n",
    "\n",
    "cifar_train = CIFAR10((\"train\",), subset=slice(None,90000))\n",
    "cifar_train_stream = DataStream.default_stream(\n",
    "    cifar_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar_train.num_examples, train_batch_size))\n",
    "\n",
    "cifar_validation = CIFAR10((\"train\",), subset=slice(90000, None))\n",
    "cifar_validation_stream = DataStream.default_stream(\n",
    "    cifar_validation, iteration_scheme=SequentialScheme(cifar_validation.num_examples, validation_batch_size))\n",
    "\n",
    "cifar_test = CIFAR10((\"test\",))\n",
    "cifar_test_stream = DataStream.default_stream(\n",
    "    cifar_test, iteration_scheme=SequentialScheme(cifar_test.num_examples, validation_batch_size))\n",
    "\n",
    "print \"The streams return batches containing %s\" % (cifar_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "\n",
    "cifar_labels = [\"airplane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create Theano variables for input and target minibatch\n",
    "input_var = T.tensor4('X')\n",
    "target_var = T.ivector('y')\n",
    "\n",
    "# create a small convolutional neural network\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax\n",
    "network = lasagne.layers.InputLayer((None, 3, 32, 32), input_var)\n",
    "network = lasagne.layers.Conv2DLayer(network, 128, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Conv2DLayer(network, 128, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Pool2DLayer(network, (2, 2), stride=2, mode='max')\n",
    "network = lasagne.layers.Conv2DLayer(network, 256, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Conv2DLayer(network, 256, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Pool2DLayer(network, (2, 2), stride=2, mode='max')\n",
    "network = lasagne.layers.Conv2DLayer(network, 512, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Conv2DLayer(network, 512, (3, 3),\n",
    "                                     nonlinearity=leaky_rectify, pad='same',\n",
    "                                     W=lasagne.init.HeNormal(gain=sqrt(2/(1.0001))))\n",
    "network = lasagne.layers.Pool2DLayer(network, (2, 2), stride=2, mode='max')\n",
    "# network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5), nonlinearity=None,\n",
    "#                                     num_units=2048, W=lasagne.init.Orthogonal())\n",
    "# network = lasagne.layers.DenseLayer(network, nonlinearity=None,\n",
    "#                                     num_units=2048, W=lasagne.init.Orthogonal())\n",
    "# network = lasagne.layers.FeaturePoolLayer(network, 16)\n",
    "network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5),\n",
    "                                   2048, nonlinearity=leaky_rectify,\n",
    "#                                    W=lasagne.init.GlorotNormal(gain=sqrt(2/(1.0001))))\n",
    "                                   W=lasagne.init.Orthogonal())\n",
    "network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5),\n",
    "                                   2048, nonlinearity=leaky_rectify,\n",
    "#                                    W=lasagne.init.GlorotNormal(gain=sqrt(2/(1.0001))))\n",
    "                                   W=lasagne.init.Orthogonal())\n",
    "network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5),\n",
    "                                    10, nonlinearity=softmax)\n",
    "\n",
    "# create loss function\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n",
    "        network, lasagne.regularization.l2)\n",
    "\n",
    "a0 = np.float32(0.02)\n",
    "tau = np.float32(cifar_train.num_examples)\n",
    "learning_rate = theano.shared(np.array(a0, dtype=config.floatX))\n",
    "t = T.scalar()\n",
    "anneal_learning_rate = theano.function([t], None, updates=[\n",
    "        (learning_rate, a0 * (tau / T.max([t, tau])))])\n",
    "\n",
    "# create parameter update expressions\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=learning_rate,\n",
    "                                            momentum=0.9)\n",
    "#updates = lasagne.updates.rmsprop(loss, params, learning_rate=0.1)\n",
    "# compile training function that updates parameters and returns training loss\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_classify = T.argmax(test_prediction, axis=1)\n",
    "test_acc = T.mean(T.eq(test_classify, target_var), dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_acc])\n",
    "classify_fn = theano.function([input_var], [test_classify])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Learning rate: 0.020000\n",
      "Epoch 1 of 10 took 494.075s\n",
      "  training loss:\t\t2.014066\n",
      "  validation accuracy:\t\t72.63 %\n",
      "After epoch 1: increased planned number of epochs to 11\n",
      "Learning rate: 0.020000\n",
      "Epoch 2 of 11 took 500.576s\n",
      "  training loss:\t\t1.506854\n",
      "  validation accuracy:\t\t79.13 %\n",
      "After epoch 2: increased planned number of epochs to 12\n",
      "Learning rate: 0.010000\n",
      "Epoch 3 of 12 took 500.674s\n",
      "  training loss:\t\t1.244388\n",
      "  validation accuracy:\t\t83.08 %\n",
      "After epoch 3: increased planned number of epochs to 13\n",
      "Learning rate: 0.006667\n",
      "Epoch 4 of 13 took 501.494s\n",
      "  training loss:\t\t1.105201\n",
      "  validation accuracy:\t\t84.42 %\n",
      "After epoch 4: increased planned number of epochs to 14\n",
      "Learning rate: 0.005000\n",
      "Epoch 5 of 14 took 501.546s\n",
      "  training loss:\t\t1.016110\n",
      "  validation accuracy:\t\t85.40 %\n",
      "After epoch 5: increased planned number of epochs to 15\n",
      "Learning rate: 0.004000\n",
      "Epoch 6 of 15 took 505.166s\n",
      "  training loss:\t\t0.946026\n",
      "  validation accuracy:\t\t85.94 %\n",
      "After epoch 6: increased planned number of epochs to 16\n",
      "Learning rate: 0.003333\n",
      "Epoch 7 of 16 took 504.641s\n",
      "  training loss:\t\t0.893145\n",
      "  validation accuracy:\t\t86.49 %\n",
      "After epoch 7: increased planned number of epochs to 17\n",
      "Learning rate: 0.002857\n",
      "Epoch 8 of 17 took 504.312s\n",
      "  training loss:\t\t0.848462\n",
      "  validation accuracy:\t\t86.99 %\n",
      "After epoch 8: increased planned number of epochs to 18\n",
      "Learning rate: 0.002500\n",
      "Epoch 9 of 18 took 504.406s\n",
      "  training loss:\t\t0.814934\n",
      "  validation accuracy:\t\t86.75 %\n",
      "Learning rate: 0.002222\n",
      "Epoch 10 of 18 took 504.614s\n",
      "  training loss:\t\t0.785004\n",
      "  validation accuracy:\t\t87.33 %\n",
      "After epoch 10: increased planned number of epochs to 20\n",
      "Learning rate: 0.002000\n",
      "Epoch 11 of 20 took 504.736s\n",
      "  training loss:\t\t0.760722\n",
      "  validation accuracy:\t\t87.36 %\n",
      "After epoch 11: increased planned number of epochs to 21\n",
      "Learning rate: 0.001818\n",
      "Epoch 12 of 21 took 504.904s\n",
      "  training loss:\t\t0.741196\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Learning rate: 0.001667\n",
      "Epoch 13 of 21 took 504.260s\n",
      "  training loss:\t\t0.723071\n",
      "  validation accuracy:\t\t87.17 %\n",
      "Learning rate: 0.001538\n",
      "Epoch 14 of 21 took 504.290s\n",
      "  training loss:\t\t0.710452\n",
      "  validation accuracy:\t\t87.59 %\n",
      "After epoch 14: increased planned number of epochs to 24\n",
      "Learning rate: 0.001429\n",
      "Epoch 15 of 24 took 503.817s\n",
      "  training loss:\t\t0.700702\n",
      "  validation accuracy:\t\t87.40 %\n",
      "Learning rate: 0.001333\n",
      "Epoch 16 of 24 took 504.014s\n",
      "  training loss:\t\t0.690578\n",
      "  validation accuracy:\t\t87.18 %\n",
      "Learning rate: 0.001250\n",
      "Epoch 17 of 24 took 504.101s\n",
      "  training loss:\t\t0.682633\n",
      "  validation accuracy:\t\t87.45 %\n",
      "Learning rate: 0.001176\n",
      "Epoch 18 of 24 took 504.427s\n",
      "  training loss:\t\t0.675566\n",
      "  validation accuracy:\t\t87.61 %\n",
      "After epoch 18: increased planned number of epochs to 28\n",
      "Learning rate: 0.001111\n",
      "Epoch 19 of 28 took 503.832s\n",
      "  training loss:\t\t0.668855\n",
      "  validation accuracy:\t\t87.58 %\n",
      "Learning rate: 0.001053\n",
      "Epoch 20 of 28 took 503.995s\n",
      "  training loss:\t\t0.665181\n",
      "  validation accuracy:\t\t87.14 %\n",
      "Learning rate: 0.001000\n",
      "Epoch 21 of 28 took 504.385s\n",
      "  training loss:\t\t0.659699\n",
      "  validation accuracy:\t\t87.67 %\n",
      "After epoch 21: increased planned number of epochs to 31\n",
      "Learning rate: 0.000952\n",
      "Epoch 22 of 31 took 504.267s\n",
      "  training loss:\t\t0.657033\n",
      "  validation accuracy:\t\t87.21 %\n",
      "Learning rate: 0.000909\n",
      "Epoch 23 of 31 took 503.860s\n",
      "  training loss:\t\t0.652111\n",
      "  validation accuracy:\t\t87.60 %\n",
      "Learning rate: 0.000870\n",
      "Epoch 24 of 31 took 504.052s\n",
      "  training loss:\t\t0.649069\n",
      "  validation accuracy:\t\t87.63 %\n",
      "Learning rate: 0.000833\n",
      "Epoch 25 of 31 took 503.866s\n",
      "  training loss:\t\t0.645741\n",
      "  validation accuracy:\t\t87.45 %\n",
      "Learning rate: 0.000800\n",
      "Epoch 26 of 31 took 503.965s\n",
      "  training loss:\t\t0.643602\n",
      "  validation accuracy:\t\t87.65 %\n",
      "Learning is stalling, reducing rate by half.\n",
      "Learning rate: 0.000385\n",
      "Epoch 27 of 31 took 504.111s\n",
      "  training loss:\t\t0.638640\n",
      "  validation accuracy:\t\t87.65 %\n",
      "Learning rate: 0.000377\n",
      "Epoch 28 of 31 took 504.345s\n",
      "  training loss:\t\t0.636613\n",
      "  validation accuracy:\t\t87.76 %\n",
      "After epoch 28: increased planned number of epochs to 42\n",
      "Learning rate: 0.000370\n",
      "Epoch 29 of 42 took 504.262s\n",
      "  training loss:\t\t0.635572\n",
      "  validation accuracy:\t\t87.93 %\n",
      "After epoch 29: increased planned number of epochs to 43\n",
      "Learning rate: 0.000364\n",
      "Epoch 30 of 43 took 503.600s\n",
      "  training loss:\t\t0.633676\n",
      "  validation accuracy:\t\t87.71 %\n",
      "Learning rate: 0.000357\n",
      "Epoch 31 of 43 took 503.496s\n",
      "  training loss:\t\t0.632738\n",
      "  validation accuracy:\t\t87.71 %\n",
      "Learning rate: 0.000351\n",
      "Epoch 32 of 43 took 502.879s\n",
      "  training loss:\t\t0.631806\n",
      "  validation accuracy:\t\t87.62 %\n",
      "Learning rate: 0.000345\n",
      "Epoch 33 of 43 took 503.801s\n",
      "  training loss:\t\t0.630296\n",
      "  validation accuracy:\t\t87.63 %\n",
      "Learning rate: 0.000339\n",
      "Epoch 34 of 43 took 503.398s\n",
      "  training loss:\t\t0.629441\n",
      "  validation accuracy:\t\t87.62 %\n",
      "Learning is stalling, reducing rate by half.\n",
      "Learning rate: 0.000167\n",
      "Epoch 35 of 43 took 503.579s\n",
      "  training loss:\t\t0.628629\n",
      "  validation accuracy:\t\t87.75 %\n",
      "Learning rate: 0.000165\n",
      "Epoch 36 of 43 took 503.235s\n",
      "  training loss:\t\t0.627899\n",
      "  validation accuracy:\t\t87.71 %\n",
      "Learning rate: 0.000164\n",
      "Epoch 37 of 43 took 503.256s\n",
      "  training loss:\t\t0.627798\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Learning rate: 0.000163\n",
      "Epoch 38 of 43 took 503.031s\n",
      "  training loss:\t\t0.627052\n",
      "  validation accuracy:\t\t87.70 %\n",
      "Learning rate: 0.000161\n",
      "Epoch 39 of 43 took 503.257s\n",
      "  training loss:\t\t0.625753\n",
      "  validation accuracy:\t\t87.83 %\n",
      "Learning is stalling, reducing rate by half.\n",
      "Learning rate: 0.000080\n",
      "Epoch 40 of 43 took 502.509s\n",
      "  training loss:\t\t0.625629\n",
      "  validation accuracy:\t\t87.84 %\n",
      "Learning rate: 0.000080\n",
      "Epoch 41 of 43 took 501.671s\n",
      "  training loss:\t\t0.625240\n",
      "  validation accuracy:\t\t87.72 %\n",
      "Learning rate: 0.000079\n",
      "Epoch 42 of 43 took 502.420s\n",
      "  training loss:\t\t0.624629\n",
      "  validation accuracy:\t\t87.72 %\n",
      "Learning rate: 0.000079\n",
      "Epoch 43 of 43 took 502.861s\n",
      "  training loss:\t\t0.624936\n",
      "  validation accuracy:\t\t87.80 %\n",
      "Model trained in 6:00:45\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = lasagne.layers.get_all_param_values(network)\n",
    "best_params_epoch = 0\n",
    "\n",
    "last_expansion_epoch = 0\n",
    "expansion_coef = 1.5\n",
    "    \n",
    "num_epochs = 10\n",
    "\n",
    "learning_rate.set_value(a0)\n",
    "\n",
    "model_start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "epoch = 0\n",
    "t = 0\n",
    "while epoch < num_epochs:\n",
    "    epoch += 1\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(\"Learning rate: {:.6f}\".format(float(learning_rate.get_value())))\n",
    "    \n",
    "    for input_batch, target_batch in cifar_train_stream.get_epoch_iterator():\n",
    "        train_err += train_fn(input_batch, target_batch.ravel())\n",
    "        train_batches += 1\n",
    "\n",
    "    t += cifar_train.num_examples\n",
    "    anneal_learning_rate(t)\n",
    "    \n",
    "    \n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for input_batch, target_batch in cifar_validation_stream.get_epoch_iterator():\n",
    "        acc = val_fn(input_batch, target_batch.ravel())[0]\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch, num_epochs, time.time() - epoch_start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "    \n",
    "    # patience expansion\n",
    "    if val_acc > best_val_acc:\n",
    "        best_params = lasagne.layers.get_all_param_values(network)\n",
    "        best_params_epoch = epoch\n",
    "        best_val_acc = val_acc\n",
    "        num_epochs = int(max(epoch * expansion_coef, epoch + 10))\n",
    "        best_val_err = val_err\n",
    "        last_expansion_epoch = epoch\n",
    "        print \"After epoch %d: increased planned number of epochs to %d\" % (epoch, num_epochs)\n",
    "\n",
    "    if epoch - last_expansion_epoch >= 5:\n",
    "        t *= 2\n",
    "        anneal_learning_rate(t)\n",
    "        last_expansion_epoch = epoch\n",
    "        print \"Learning is stalling, reducing rate by half.\"\n",
    "\n",
    "model_end_time = datetime.now().replace(microsecond=0)\n",
    "        \n",
    "print \"Model trained in {}\".format(model_end_time - model_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t87.43 %\n",
      "Final results (using best_params from epoch 29):\n",
      "  test accuracy:\t\t87.37 %\n"
     ]
    }
   ],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for input_batch, target_batch in cifar_test_stream.get_epoch_iterator():\n",
    "    acc = val_fn(input_batch, target_batch.ravel())[0]\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "\n",
    "\n",
    "lasagne.layers.set_all_param_values(network, best_params)\n",
    "\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for input_batch, target_batch in cifar_test_stream.get_epoch_iterator():\n",
    "    acc = val_fn(input_batch, target_batch.ravel())[0]\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results (using best_params from epoch {}):\".format(best_params_epoch))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lasagne.layers.set_all_param_values(network, best_params)\n",
    "\n",
    "def print_results():\n",
    "    time = datetime.now()\n",
    "    f = open('results-{}-{}-{}.txt'.format(time.hour, time.minute, time.second),'w')\n",
    "    for input_batch, target_batch in cifar_test_stream.get_epoch_iterator():\n",
    "        predicted = classify_fn(input_batch)[0]\n",
    "        for prediction, target in zip(predicted, target_batch.ravel()):\n",
    "            f.write('{} {}\\n'.format(cifar_labels[prediction], cifar_labels[target]))\n",
    "        \n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
